<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-01-19T14:57:32+08:00</updated><id>http://localhost:4000/</id><title type="html">Coding Girl</title><subtitle>坚持记录学习和工作的点点滴滴，不积跬步无以至千里。</subtitle><entry><title type="html">Coursera ML Learning Notes</title><link href="http://localhost:4000/machine/learning/2017/12/19/coursera-ml-notes-w1.html" rel="alternate" type="text/html" title="Coursera ML Learning Notes" /><published>2017-12-19T23:06:11+08:00</published><updated>2017-12-19T23:06:11+08:00</updated><id>http://localhost:4000/machine/learning/2017/12/19/coursera-ml-notes-w1</id><content type="html" xml:base="http://localhost:4000/machine/learning/2017/12/19/coursera-ml-notes-w1.html">&lt;h2 id=&quot;1-first-week&quot;&gt;1. First Week&lt;/h2&gt;

&lt;h3 id=&quot;11-introduction&quot;&gt;1.1 Introduction&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Supervised Learning: Given a data set and correct output, find the relationship between the intput and output.
    &lt;ul&gt;
      &lt;li&gt;Regression: Predict results within a continuous output&lt;/li&gt;
      &lt;li&gt;Classification: Predict results in a discrete output&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unsupervised Learning: Derive the structure from data, no feedback based on prediciton results.
    &lt;ul&gt;
      &lt;li&gt;Clustering: Find a way to automatically group the data&lt;/li&gt;
      &lt;li&gt;Non-clustering: “Cocktail Party Algorithm”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;12-model-and-cost-function&quot;&gt;1.2 Model and Cost Function&lt;/h3&gt;
&lt;p&gt;Cost Function: Used to measure the accuracy of our hypothesis function. It’s also called ‘Squared Error Function’.
&lt;img src=&quot;http://localhost:4000/assets/cost_function.png&quot; alt=&quot;Cost Function&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;13-parameter-learning&quot;&gt;1.3 Parameter Learning&lt;/h3&gt;
&lt;p&gt;Gradient Descent Algorithm is used to estimate the parameters in hypothesis function.
&lt;img src=&quot;http://localhost:4000/assets/gradient_descent_algorithm.png&quot; alt=&quot;Gradient Descent Algorithm&quot; /&gt;
Alpha is ‘Learning Rate’ and Updating the theta 0 and theta 1 simultaneously is important.&lt;/p&gt;

&lt;p&gt;Gradient Descent can converge to a local minimum, even with the learning rate fixed. As we approach a local minimum, gradient descent will automatically take smaller steps. So, no need to decrease alpha over time.
&lt;img src=&quot;http://localhost:4000/assets/gradient_descent_for_linear_regression.png&quot; alt=&quot;Gradient Descent For Linear Regression&quot; /&gt;
When we use every example in the entire training set on every step, it’s called Batch Gradient descent.&lt;/p&gt;

&lt;h2 id=&quot;2-second-week&quot;&gt;2. Second Week&lt;/h2&gt;
&lt;h3 id=&quot;21-multivariate-linear-regression&quot;&gt;2.1 Multivariate Linear Regression&lt;/h3&gt;
&lt;p&gt;Multivariable denotion
&lt;img src=&quot;http://localhost:4000/assets/multivariable_denotion.png&quot; alt=&quot;Multivariable denotion&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Multivariable hypothesis
&lt;img src=&quot;http://localhost:4000/assets/multivariable_hypothesis.png&quot; alt=&quot;Multivariable hypothesis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Multivariable hypothesis Function
&lt;img src=&quot;http://localhost:4000/assets/multivariable_hypothesis_function.png&quot; alt=&quot;Multivariable hypothesis Function&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;22-gradient-descent-for-multiple-variables&quot;&gt;2.2 Gradient Descent for Multiple Variables&lt;/h3&gt;
&lt;p&gt;Gradient Descent for Multiple Variables
&lt;img src=&quot;http://localhost:4000/assets/gradient_descent_for_multiple_variables.png&quot; alt=&quot;Gradient Descent for Multiple Variables&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;23-feature-scaling&quot;&gt;2.3 Feature Scaling&lt;/h3&gt;
&lt;p&gt;Having the input values in the same range helps speeding up gradient descent and so we have two method: feature scaling and mean normalization.
Feature Scaling: Divide the input value by the range - &amp;gt; [0, 1]
Mean Normalization:
&lt;img src=&quot;http://localhost:4000/assets/mean_normalization.png&quot; alt=&quot;Mean Normalization&quot; /&gt;
μ is the average of the values for feature
s is the range of values (max - min), it means standard deviation.&lt;/p&gt;

&lt;h3 id=&quot;24-learning-rate&quot;&gt;2.4 Learning Rate&lt;/h3&gt;
&lt;p&gt;It has been proven that if learning rate α is sufficiently small, then J(θ) will decrease on every iteration.
If α is too small: slow convergence.
If α is too large: ￼may not decrease on every iteration and thus may not converge.&lt;/p&gt;</content><author><name></name></author><summary type="html">1. First Week</summary></entry></feed>